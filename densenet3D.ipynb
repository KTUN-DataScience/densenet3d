{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b1a76199bd0a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Project imports\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "#Project imports\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import shutil\n",
    "import torch.nn as nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from datetime import datetime\n",
    "from collections import OrderedDict\n",
    "from utils.target_transforms import ClassLabel, VideoID\n",
    "from utils.temporal_transforms import TemporalBeginCrop, TemporalCenterCrop, TemporalRandomCrop\n",
    "from utils.spatial_transforms import Normalize, Scale, CenterCrop, CornerCrop, Compose, ToTensor, TemporalRandomCrop, MultiScaleCornerCrop, MultiScaleRandomCrop, RandomHorizontalFlip\n",
    "from dataset.jester import Jester\n",
    "from utils.train_utils import Logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration setting for the entire notebook will be handled here\n",
    "class Path:\n",
    "    \"\"\"\n",
    "    Project constant path class\n",
    "    -----\n",
    "    Attributes:\n",
    "\n",
    "    - dataset_path (str): Path to dataset \n",
    "\n",
    "    - annotation_path (str): Path to dataset annotation {validation and training list labels}\n",
    "\n",
    "    - result_path (str): Store model results and weights to be dumped in this folder\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset_path = 'dataset/jester'\n",
    "\n",
    "    annotation_path = 'dataset/annotation/jester.json'\n",
    "\n",
    "    result_path = 'results'\n",
    "\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    Model training Configuration class\n",
    "    -----\n",
    "\n",
    "    Attributes:\n",
    "\n",
    "    - arch: Name of architecture being used\n",
    "    - learning_rate (float) - how many filters to add each layer (k in paper)\n",
    "    - n_epoch (int) - how many layers in each pooling block\n",
    "    - lr_patience:\n",
    "    - momentum:\n",
    "    - begin_epoch:\n",
    "    - n_epochs:\n",
    "    - lr_steps:\n",
    "    - weight_decay:\n",
    "    - nesterov:\n",
    "    - dataset:\n",
    "    - model:\n",
    "    - width_mult:\n",
    "    - batch_size:\n",
    "    - n_threads:\n",
    "    -  dampening:\n",
    "    - modality:\n",
    "    - sample_duration:\n",
    "    - sample_size:\n",
    "    - n_val_samples:\n",
    "    - downsample:\n",
    "    - optimiser:\n",
    "    - cuda:\n",
    "    - dataset_path:\n",
    "    - annotation_path:\n",
    "    - result_path:\n",
    "    - n_classes:\n",
    "    - initial_scale:\n",
    "    - n_scales:\n",
    "    - scale_step:\n",
    "    - norm_value:\n",
    "    - mean_dataset:\n",
    "    - mean_norm:\n",
    "    - std_norm: \n",
    "\n",
    "    Usage:\n",
    "    ------\n",
    "\n",
    "        from config import Config\n",
    "        ``Config.learning_rate``\n",
    "    \"\"\"\n",
    "  \n",
    "\n",
    "    arch = 'Densenet'\n",
    "\n",
    "    arch_type = 161\n",
    "\n",
    "    learning_rate = 0.1\n",
    "\n",
    "    lr_patience = 10\n",
    "\n",
    "    momentum = 0.9\n",
    "\n",
    "    begin_epoch = 1\n",
    "\n",
    "    n_epochs = 100\n",
    "\n",
    "    n_classes = 27\n",
    "\n",
    "    lr_steps = [25, 40, 55, 70] # (lr_rate * (0.1 ** step_position ))  =>  (0.1 (0.1 ** 1))\n",
    "\n",
    "    weight_decay = 1e-3\n",
    "\n",
    "    nesterov = False\n",
    "\n",
    "    dataset = 'jester'\n",
    "\n",
    "    model = 'densenet3d'\n",
    "\n",
    "    activation = 'relu' # options: 'relu' or 'leaky_relu'\n",
    "\n",
    "    classifier = 'Linear' # options: 'Softmax' or 'Linear'\n",
    "\n",
    "    negative_slope = 0.1\n",
    "    \n",
    "    train = True\n",
    "\n",
    "    validation = True\n",
    "\n",
    "    test = False\n",
    "\n",
    "    seed = 1000\n",
    "\n",
    "    width_mult = 1\n",
    "\n",
    "    batch_size = 16\n",
    "\n",
    "    n_threads = 16\n",
    "\n",
    "    dampening = 0.9\n",
    "\n",
    "    modality = 'RGB'\n",
    "\n",
    "    sample_duration = 16\n",
    "\n",
    "    sample_size = 112\n",
    "\n",
    "    n_val_samples = 1\n",
    "\n",
    "    downsample = 2\n",
    "\n",
    "    optimizer = 'SGD' # options: 'Adam' or 'SGD'\n",
    "\n",
    "    betas = (0.9, 0.999)\n",
    "\n",
    "    eps = 1e-8\n",
    "\n",
    "    amsgrad = False\n",
    "\n",
    "    cuda = True\n",
    "\n",
    "    dataset_path = Path.dataset_path\n",
    "\n",
    "    annotation_path = Path.annotation_path\n",
    "\n",
    "    result_path = Path.result_path\n",
    "\n",
    "    resume_path = ''\n",
    "\n",
    "    initial_scale = 1\n",
    "\n",
    "    n_scales = 5\n",
    "\n",
    "    scale_step = 0.84089641525\n",
    "\n",
    "    norm_value = 255\n",
    "\n",
    "    mean_dataset = 'activitynet'\n",
    "\n",
    "    dataset = ''\n",
    "\n",
    "    store_name = ''\n",
    "\n",
    "    train_crop = 'random'\n",
    "\n",
    "    mean_norm = False,\n",
    "\n",
    "    std_norm = False,\n",
    "\n",
    "    softmax_in_test = True\n",
    "\n",
    "    scale_in_test = 1.0\n",
    "\n",
    "    test_subset = 'val'\n",
    "\n",
    "    crop_position_in_test = 'c'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denset3D Model \n",
    "class _DenseLayer(nn.Sequential):\n",
    "    \n",
    "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n",
    "        super().__init__()\n",
    "        self.add_module('norm1', nn.BatchNorm3d(num_input_features))\n",
    "        self.add_module('relu1', set_activation())\n",
    "        self.add_module(\n",
    "            'conv1',\n",
    "            nn.Conv3d(num_input_features,\n",
    "                      bn_size * growth_rate,\n",
    "                      kernel_size=1,\n",
    "                      stride=1,\n",
    "                      bias=False))\n",
    "        self.add_module('norm2', nn.BatchNorm3d(bn_size * growth_rate))\n",
    "        self.add_module('relu2', set_activation())\n",
    "        self.add_module(\n",
    "            'conv2',\n",
    "            nn.Conv3d(bn_size * growth_rate,\n",
    "                      growth_rate,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1,\n",
    "                      bias=False))\n",
    "        self.drop_rate = drop_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        new_features = super().forward(x)\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features,\n",
    "                                     p=self.drop_rate,\n",
    "                                     training=self.training)\n",
    "        return torch.cat([x, new_features], 1)\n",
    "\n",
    "\n",
    "class _DenseBlock(nn.Sequential):\n",
    "\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate,\n",
    "                 drop_rate):\n",
    "        super().__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayer(num_input_features + i * growth_rate,\n",
    "                                growth_rate, bn_size, drop_rate)\n",
    "            self.add_module('denselayer{}'.format(i + 1), layer)\n",
    "\n",
    "\n",
    "class _Transition(nn.Sequential):\n",
    "\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super().__init__()\n",
    "        self.add_module('norm', nn.BatchNorm3d(num_input_features))\n",
    "        self.add_module('relu', set_activation())\n",
    "        self.add_module(\n",
    "            'conv',\n",
    "            nn.Conv3d(num_input_features,\n",
    "                      num_output_features,\n",
    "                      kernel_size=1,\n",
    "                      stride=1,\n",
    "                      bias=False))\n",
    "        self.add_module('pool', nn.AvgPool3d(kernel_size=2, stride=2))\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    \"\"\"Densenet-BC model class\n",
    "    Args:\n",
    "        growth_rate (int) - how many filters to add each layer (k in paper)\n",
    "        block_config (list of 4 ints) - how many layers in each pooling block\n",
    "        num_init_features (int) - the number of filters to learn in the first convolution layer\n",
    "        bn_size (int) - multiplicative factor for number of bottle neck layers\n",
    "          (i.e. bn_size * k features in the bottleneck layer)\n",
    "        drop_rate (float) - dropout rate after each dense layer\n",
    "        num_classes (int) - number of classification classes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_input_channels=3,\n",
    "                 conv1_t_size=7,\n",
    "                 conv1_t_stride=1,\n",
    "                 no_max_pool=False,\n",
    "                 growth_rate=32,\n",
    "                 block_config=(6, 12, 24, 16),\n",
    "                 num_init_features=64,\n",
    "                 bn_size=4,\n",
    "                 drop_rate=0,\n",
    "                 num_classes=1000):\n",
    "    \n",
    "        super().__init__()\n",
    "\n",
    "        # First convolution\n",
    "        self.features = [('conv1',\n",
    "                          nn.Conv3d(n_input_channels,\n",
    "                                    num_init_features,\n",
    "                                    kernel_size=(conv1_t_size, 7, 7),\n",
    "                                    stride=(conv1_t_stride, 2, 2),\n",
    "                                    padding=(conv1_t_size // 2, 3, 3),\n",
    "                                    bias=False)),\n",
    "                         ('norm1', nn.BatchNorm3d(num_init_features)),\n",
    "                         ('relu1', set_activation())]\n",
    "        if not no_max_pool:\n",
    "            self.features.append(\n",
    "                ('pool1', nn.MaxPool3d(kernel_size=3, stride=2, padding=1)))\n",
    "        self.features = nn.Sequential(OrderedDict(self.features))\n",
    "\n",
    "        # Each denseblock\n",
    "        num_features = num_init_features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = _DenseBlock(num_layers=num_layers,\n",
    "                                num_input_features=num_features,\n",
    "                                bn_size=bn_size,\n",
    "                                growth_rate=growth_rate,\n",
    "                                drop_rate=drop_rate)\n",
    "            self.features.add_module('denseblock{}'.format(i + 1), block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            if i != len(block_config) - 1:\n",
    "                trans = _Transition(num_input_features=num_features,\n",
    "                                    num_output_features=num_features // 2)\n",
    "                self.features.add_module('transition{}'.format(i + 1), trans)\n",
    "                num_features = num_features // 2\n",
    "\n",
    "        # Final batch norm\n",
    "        self.features.add_module('norm5', nn.BatchNorm3d(num_features))\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                m.weight = nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "            elif isinstance(m, nn.BatchNorm3d) or isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "        # Linear layer\n",
    "        if Config.classifier == 'Softmax' or Config.classifier == 'LogSoftmax':\n",
    "            self.classifier = nn.LogSoftmax()\n",
    "        else:\n",
    "            self.classifier = nn.Linear(num_features, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                nn.init.kaiming_normal_(m.weight,\n",
    "                                        mode='fan_out',\n",
    "                                        nonlinearity=Config.activation)\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        activation_method = getattr(F, Config.activation)\n",
    "        out = activation_method(features, inplace=True)\n",
    "        out = F.adaptive_avg_pool3d(out,\n",
    "                                    output_size=(1, 1,\n",
    "                                                 1)).view(features.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def getModel(**kwargs):\n",
    "    if Config.arch_type==121:\n",
    "        return DenseNet(num_init_features=64,\n",
    "                         growth_rate=32,\n",
    "                         block_config=(6, 12, 24, 16), **kwargs)\n",
    "    elif Config.arch_type==161:\n",
    "        return DenseNet(num_init_features=96,\n",
    "                         growth_rate=48,\n",
    "                         block_config=(6, 12, 36, 24), **kwargs)\n",
    "    elif Config.arch_type==169:\n",
    "        return DenseNet(num_init_features=64,\n",
    "                         growth_rate=32,\n",
    "                         block_config=(6, 12, 32, 32), **kwargs)\n",
    "    elif Config.arch_type==201:\n",
    "        return DenseNet(num_init_features=64,\n",
    "                         growth_rate=32,\n",
    "                         block_config=(6, 12, 48, 32), **kwargs)\n",
    "\n",
    "\n",
    "def set_activation():\n",
    "    assert(Config.activation =='leaky_relu' or Config.activation == 'relu')\n",
    "    if Config.activation == 'leaky_relu':\n",
    "        return nn.LeakyReLU(Config.negative_slope, inplace=True)\n",
    "    return nn.ReLU(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility Functions\n",
    "def get_mean(norm_value=255, dataset='activitynet'):\n",
    "    assert dataset in ['activitynet', 'kinetics']\n",
    "\n",
    "    if dataset == 'activitynet':\n",
    "        return [\n",
    "            114.7748 / norm_value, 107.7354 / norm_value, 99.4750 / norm_value\n",
    "        ]\n",
    "    elif dataset == 'kinetics':\n",
    "        # Kinetics (10 videos for each class)\n",
    "        return [\n",
    "            110.63666788 / norm_value, 103.16065604 / norm_value,\n",
    "            96.29023126 / norm_value\n",
    "        ]\n",
    "\n",
    "def get_std(norm_value=255):\n",
    "        # Kinetics (10 videos for each class)\n",
    "    return [\n",
    "        38.7568578 / norm_value, 37.88248729 / norm_value,\n",
    "        40.02898126 / norm_value\n",
    "    ]\n",
    "\n",
    "def set_norm_method(mean, std):\n",
    "    \"\"\"\n",
    "    Set normalization method based on mean and std\n",
    "\n",
    "    Parameters:\n",
    "    ------------\n",
    "    mean : float \n",
    "        Mean value for dataset\n",
    "    std : float \n",
    "        Standard deviation for dataset\n",
    "    \n",
    "    Returns\n",
    "    ---------        \n",
    "    Normalize\n",
    "        Normalise class\n",
    "\n",
    "    \"\"\"\n",
    "    if Config.mean_norm and not Config.std_norm:\n",
    "        return Normalize([0, 0, 0], [1, 1, 1])\n",
    "    elif not Config.std_norm:\n",
    "        return Normalize(mean, [1, 1, 1])\n",
    "    else: \n",
    "        return Normalize(mean, std) \n",
    "\n",
    "def set_optimizer(model):\n",
    "    \"\"\"\n",
    "    Set optimizer algorithm:\n",
    "\n",
    "    Args:\n",
    "    ----\n",
    "    model: torch.nn.Module\n",
    "        Instance of nn.Module\n",
    "    \n",
    "    Returns: \n",
    "    --------\n",
    "    torch.optim.Optimizer\n",
    "    \"\"\"\n",
    "    assert(Config.optimizer == 'SGD' or Config.optimizer == 'Adam')\n",
    "\n",
    "    if Config.optimizer == 'SGD':\n",
    "        if Config.nesterov:\n",
    "            dampening = 0\n",
    "        else:\n",
    "            dampening = Config.dampening\n",
    "\n",
    "        return optim.SGD(model.parameters(), \n",
    "            lr=Config.learning_rate, \n",
    "            momentum=Config.momentum,\n",
    "            dampening=dampening,\n",
    "            weight_decay=Config.weight_decay,\n",
    "            nesterov=Config.nesterov)\n",
    "\n",
    "    return optim.Adam(model.parameters(), \n",
    "        lr=Config.learning_rate, \n",
    "        betas=Config.betas,\n",
    "        eps=Config.eps,\n",
    "        weight_decay=Config.weight_decay,\n",
    "        amsgrad=Config.amsgrad)\n",
    "\n",
    "def set_crop_method(scales):\n",
    "    \"\"\"\n",
    "    Set cropping method to be applied on the dataset in memory\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "    scales : list \n",
    "        list of scaling ratios\n",
    "\n",
    "    Returns:\n",
    "    ---------\n",
    "    Spatial Transformation\n",
    "\n",
    "    \"\"\"\n",
    "    assert Config.train_crop in ['random','corner', 'center']\n",
    "    crop = Config.train_crop\n",
    "    if crop == 'random':\n",
    "        return MultiScaleRandomCrop(scales, Config.sample_size)\n",
    "    elif crop == 'corner':\n",
    "        return MultiScaleCornerCrop(scales, Config.sample_size)\n",
    "    elif crop == 'center':\n",
    "        return MultiScaleCornerCrop(scales, Config.sample_size, crop_positions=['c'])\n",
    "\n",
    "\n",
    "def get_training_set(spatial_transform, temporal_transform,\n",
    "                     target_transform):\n",
    "    return Jester(\n",
    "        Config.dataset_path,\n",
    "        Config.annotation_path,\n",
    "        'training',\n",
    "        spatial_transform=spatial_transform,\n",
    "        temporal_transform=temporal_transform,\n",
    "        target_transform=target_transform,\n",
    "        sample_duration=Config.sample_duration)\n",
    "\n",
    "def get_validation_set(spatial_transform, temporal_transform,\n",
    "                       target_transform):\n",
    "        return Jester(\n",
    "            Config.dataset_path,\n",
    "            Config.annotation_path,\n",
    "            'validation',\n",
    "            Config.n_val_samples,\n",
    "            spatial_transform,\n",
    "            temporal_transform,\n",
    "            target_transform,\n",
    "            sample_duration=Config.sample_duration)\n",
    "\n",
    "def save_checkpoint(state, is_best, store_name):\n",
    "    torch.save(state, '%s/%s_checkpoint.pth' % (Config.result_path, store_name))\n",
    "    if is_best:\n",
    "        shutil.copyfile('%s/%s_checkpoint.pth' % (Config.result_path, store_name),'%s/%s_best.pth' % (Config.result_path, Config.store_name))\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr_new = Config.learning_rate * (0.1 ** (sum(epoch >= np.array(Config.lr_steps))))\n",
    "    for param_group in optimizer.param_groups:\n",
    "          param_group['lr'] = lr_new\n",
    "        #param_group['lr'] = opt.learning_rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def calculate_accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "    \n",
    "def train_epoch(epoch, data_loader, model, criterion, optimizer,\n",
    "                epoch_logger, batch_logger):\n",
    "    print('Training at epoch {}/{}'.format(epoch, Config.n_epochs))\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    end_time = time.time()\n",
    "    for i, (inputs, targets) in enumerate(data_loader):\n",
    "        data_time.update(time.time() - end_time)\n",
    "\n",
    "        if Config.cuda:\n",
    "            targets = targets.cuda()\n",
    "        inputs = Variable(inputs)\n",
    "        targets = Variable(targets)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        losses.update(loss.data, inputs.size(0))\n",
    "        prec1, prec5 = calculate_accuracy(outputs.data, targets.data, topk=(1,5))\n",
    "        top1.update(prec1, inputs.size(0))\n",
    "        top5.update(prec5, inputs.size(0))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_time.update(time.time() - end_time)\n",
    "        end_time = time.time()\n",
    "\n",
    "        batch_logger.log({\n",
    "            'epoch': epoch,\n",
    "            'batch': i + 1,\n",
    "            'iter': (epoch - 1) * len(data_loader) + (i + 1),\n",
    "            'loss': losses.val.item(),\n",
    "            'prec1': top1.val.item(),\n",
    "            'prec5': top5.val.item(),\n",
    "            'lr': optimizer.param_groups[0]['lr']\n",
    "        })\n",
    "        if i % 10 == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t lr: {lr:.5f}\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.5f} ({top1.avg:.5f})\\t'\n",
    "                  'Prec@5 {top5.val:.5f} ({top5.avg:.5f})'.format(\n",
    "                      epoch,\n",
    "                      i,\n",
    "                      len(data_loader),\n",
    "                      batch_time=batch_time,\n",
    "                      data_time=data_time,\n",
    "                      loss=losses,\n",
    "                      top1=top1,\n",
    "                      top5=top5,\n",
    "                      lr=optimizer.param_groups[0]['lr']))\n",
    "\n",
    "    epoch_logger.log({\n",
    "        'epoch': epoch,\n",
    "        'loss': losses.avg.item(),\n",
    "        'prec1': top1.avg.item(),\n",
    "        'prec5': top5.avg.item(),\n",
    "        'lr': optimizer.param_groups[0]['lr']\n",
    "    })\n",
    "\n",
    "    #if epoch % opt.checkpoint == 0:\n",
    "    #    save_file_path = os.path.join(opt.result_path,\n",
    "    #                                  'save_{}.pth'.format(epoch))\n",
    "    #    states = {\n",
    "    #        'epoch': epoch + 1,\n",
    "    #        'arch': opt.arch,\n",
    "    #        'state_dict': model.state_dict(),\n",
    "    #        'optimizer': optimizer.state_dict(),\n",
    "    #    }\n",
    "    #    torch.save(states, save_file_path)\n",
    "\n",
    "\n",
    "# TODO: Try using CTross validation.\n",
    "\n",
    "def val_epoch(epoch, data_loader, model, criterion, logger):\n",
    "    print('validation at epoch {}'.format(epoch))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    end_time = time.time()\n",
    "    for i, (inputs, targets) in enumerate(data_loader):\n",
    "        data_time.update(time.time() - end_time)\n",
    "\n",
    "        if Config.cuda:\n",
    "            targets = targets.cuda()\n",
    "        with torch.no_grad():\n",
    "            inputs = Variable(inputs)\n",
    "            targets = Variable(targets)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        prec1, prec5 = calculate_accuracy(outputs.data, targets.data, topk=(1,5))\n",
    "        top1.update(prec1, inputs.size(0))\n",
    "        top5.update(prec5, inputs.size(0))\n",
    "\n",
    "        losses.update(loss.data, inputs.size(0))\n",
    "\n",
    "        batch_time.update(time.time() - end_time)\n",
    "        end_time = time.time()\n",
    "\n",
    "        print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "              'Time {batch_time.val:.5f} ({batch_time.avg:.5f})\\t'\n",
    "              'Data {data_time.val:.5f} ({data_time.avg:.5f})\\t'\n",
    "              'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "              'Prec@1 {top1.val:.5f} ({top1.avg:.5f})\\t'\n",
    "              'Prec@5 {top5.val:.5f} ({top5.avg:.5f})'.format(\n",
    "                  epoch,\n",
    "                  i + 1,\n",
    "                  len(data_loader),\n",
    "                  batch_time=batch_time,\n",
    "                  data_time=data_time,\n",
    "                  loss=losses,\n",
    "                  top1=top1,\n",
    "                  top5=top5))\n",
    "\n",
    "    logger.log({'epoch': epoch,\n",
    "                'loss': losses.avg.item(),\n",
    "                'prec1': top1.avg.item(),\n",
    "                'prec5': top5.avg.item()})\n",
    "\n",
    "    return losses.avg.item(), top1.avg.item()\n",
    "\n",
    "\n",
    "def calculate_video_results(output_buffer, video_id, test_results, class_names):\n",
    "    video_outputs = torch.stack(output_buffer)\n",
    "    average_scores = torch.mean(video_outputs, dim=0)\n",
    "    sorted_scores, locs = torch.topk(average_scores, k=10)\n",
    "\n",
    "    video_results = []\n",
    "    for i in range(sorted_scores.size(0)):\n",
    "        video_results.append({\n",
    "            'label': class_names[int(locs[i])],\n",
    "            'score': float(sorted_scores[i])\n",
    "        })\n",
    "\n",
    "    test_results['results'][video_id] = video_results\n",
    "\n",
    "\n",
    "def test(data_loader, model, class_names):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "\n",
    "    end_time = time.time()\n",
    "    output_buffer = []\n",
    "    previous_video_id = ''\n",
    "    test_results = {'results': {}}\n",
    "    for i, (inputs, targets) in enumerate(data_loader):\n",
    "        data_time.update(time.time() - end_time)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = Variable(inputs)\n",
    "        outputs = model(inputs)\n",
    "        if Config.softmax_in_test:\n",
    "            outputs = F.softmax(outputs, dim=1)\n",
    "\n",
    "        for j in range(outputs.size(0)):\n",
    "            if not (i == 0 and j == 0) and targets[j] != previous_video_id:\n",
    "                calculate_video_results(output_buffer, previous_video_id,\n",
    "                                        test_results, class_names)\n",
    "                output_buffer = []\n",
    "            output_buffer.append(outputs[j].data.cpu())\n",
    "            previous_video_id = targets[j]\n",
    "\n",
    "        if (i % 100) == 0:\n",
    "            with open(\n",
    "                    os.path.join(Config.result_path, '{}.json'.format(\n",
    "                        Config.test_subset)), 'w') as f:\n",
    "                json.dump(test_results, f)\n",
    "\n",
    "        batch_time.update(time.time() - end_time)\n",
    "        end_time = time.time()\n",
    "\n",
    "        print('[{}/{}]\\t'\n",
    "              'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "              'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'.format(\n",
    "                  i + 1,\n",
    "                  len(data_loader),\n",
    "                  batch_time=batch_time,\n",
    "                  data_time=data_time))\n",
    "    with open(\n",
    "            os.path.join(Config.result_path, '{}.json'.format(Config.test_subset)),\n",
    "            'w') as f:\n",
    "        json.dump(test_results, f)\n",
    "\n",
    "def evaluate_model(model):\n",
    "    torch.manual_seed(Config.seed)\n",
    "    model = getModel(Config.n_classes).cuda()\n",
    "    model = nn.DataParallel(model, device_ids=None)\n",
    "    params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(params);\n",
    "    mean = get_mean(Config.norm_value, dataset = Config.mean_dataset)\n",
    "    std = get_std(Config.norm_value)\n",
    "    norm_method = set_norm_method(mean, std)\n",
    "\n",
    "    spatial_transform = Compose([\n",
    "        Scale(int(Config.sample_size / Config.scale_in_test)),\n",
    "        CornerCrop(Config.sample_size, Config.crop_position_in_test),\n",
    "        ToTensor(Config.norm_value), norm_method\n",
    "    ])\n",
    "\n",
    "    temporal_transform = TemporalRandomCrop(Config.sample_duration, Config.downsample)\n",
    "    \n",
    "    target_transform = ClassLabel()\n",
    "\n",
    "    # test_data = get_test_set(spatial_transform, temporal_transform, target_transform)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "            test_data,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            num_workers=Config.n_threads,\n",
    "            pin_memory=True)\n",
    "    model.eval()\n",
    "\n",
    "    recorder = []\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for i, (inputs, targets) in enumerate(test_loader):\n",
    "        if Config.cuda:\n",
    "            targets = targets.cuda(async=True)\n",
    "        #inputs = Variable(torch.squeeze(inputs), volatile=True)\n",
    "        inputs = Variable(inputs, volatile=True)\n",
    "        targets = Variable(targets, volatile=True)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        recorder.append(outputs.data.cpu().numpy().copy())\n",
    "        #outputs = torch.unsqueeze(torch.mean(outputs, 0), 0)\n",
    "        prec1, prec5 = calculate_accuracy(outputs, targets, topk=(1, 5))\n",
    "\n",
    "        top1.update(prec1, inputs.size(0))\n",
    "        top5.update(prec5, inputs.size(0))\n",
    "\n",
    "        batch_time.update(time.time() - start)\n",
    "        end_time = time.time()\n",
    "\n",
    "        print('[{0}/{1}]\\t'\n",
    "            'Time {batch_time.val:.5f} ({batch_time.avg:.5f})\\t'\n",
    "            'prec@1 {top1.avg:.5f} prec@5 {top5.avg:.5f}'.format(\n",
    "                i + 1,\n",
    "                len(test_loader),\n",
    "                batch_time=batch_time,\n",
    "                top1=top1,\n",
    "                top5=top5))\n",
    "\n",
    "    video_pred = [np.argmax(np.mean(x, axis=0)) for x in recorder]\n",
    "    print(video_pred)\n",
    "\n",
    "    with open('dataset/annotation/categories.txt') as f:\n",
    "        lines = f.readlines()\n",
    "        categories = [item.rstrip() for item in lines]\n",
    "\n",
    "    name_list = [x.strip().split()[0] for x in open('dataset/annotation/vallist.txt')]\n",
    "\n",
    "    order_dict = {e:i for i, e in enumerate(sorted(name_list))}\n",
    "    reorder_output = [None] * len(recorder)\n",
    "    reorder_pred = [None] * len(recorder)\n",
    "    output_csv = []\n",
    "\n",
    "    for i in range(len(recorder)):\n",
    "        idx = order_dict[name_list[i]]\n",
    "        reorder_output[idx] = recorder[i]\n",
    "        reorder_pred[idx] = video_pred[i]\n",
    "        output_csv.append('%s;%s'%(name_list[i],\n",
    "                                categories[video_pred[i]]))\n",
    "\n",
    "        with open(Config.result_path +'/'+Config.dataset + '_predictions.csv','w') as f:\n",
    "            f.write('\\n'.join(output_csv))\n",
    " \n",
    "    print('-----Evaluation is finished------')\n",
    "    print('Overall Prec@1 {:.05f}% Prec@5 {:.05f}%'.format(top1.avg, top5.avg))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Code execution begin here\n",
    "start =  datetime.now()\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    \n",
    "print(f'Graphic Cart Used for the experiment: {device}')\n",
    "\n",
    "# Initialize model\n",
    "model = getModel(Config.n_classes).cuda()\n",
    "model = nn.DataParallel(model, device_ids=None)\n",
    "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(params);\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = get_mean(Config.norm_value, dataset = Config.mean_dataset) \n",
    "std = get_std(Config.norm_value)\n",
    "store_name = '_'.join([Config.dataset, Config.model, str(Config.width_mult) + 'x',\n",
    "                               Config.modality, str(Config.sample_duration)])\n",
    "\n",
    "norm_method = set_norm_method(mean, std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # set scaling values\n",
    "scales = [Config.initial_scale]\n",
    "for i in range(1, Config.n_scales):\n",
    "    scales.append(scales[-1] * Config.scale_step)\n",
    "\n",
    "optimizer = set_optimizer(model)\n",
    "\n",
    "\n",
    "if Config.train:\n",
    "    crop_method = set_crop_method(scales)\n",
    "\n",
    "    spatial_transform = Compose([\n",
    "        RandomHorizontalFlip(),\n",
    "        #RandomRotate(),\n",
    "        #RandomResize(),\n",
    "        crop_method,\n",
    "        #MultiplyValues(),\n",
    "        #Dropout(),\n",
    "        #SaltImage(),\n",
    "        #Gaussian_blur(),\n",
    "        #SpatialElasticDisplacement(),\n",
    "        ToTensor(Config.norm_value), norm_method\n",
    "    ])\n",
    "    temporal_transform = TemporalRandomCrop(Config.sample_duration, Config.downsample)\n",
    "    target_transform = ClassLabel()\n",
    "    training_data = get_training_set(spatial_transform,temporal_transform,target_transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "            training_data,\n",
    "            batch_size=Config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=Config.n_threads,\n",
    "            pin_memory=True)\n",
    "\n",
    "    train_logger = Logger(\n",
    "            os.path.join(Config.result_path, 'DenseNet'+ str(Config.arch_type) +'train.log'),\n",
    "            ['epoch', 'loss', 'prec1', 'prec5', 'lr'])\n",
    "            \n",
    "    train_batch_logger = Logger(\n",
    "        os.path.join(Config.result_path, 'DenseNet'+ str(Config.arch_type) + 'train_batch.log'),\n",
    "        ['epoch', 'batch', 'iter', 'loss', 'prec1', 'prec5', 'lr'])\n",
    "\n",
    "    # Set optimizer algorithm\n",
    "\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', \n",
    "        patience=Config.lr_patience)\n",
    "\n",
    "# If set validation set. Apply spatial transformations. \n",
    "if Config.validation:\n",
    "    spatial_transform = Compose([\n",
    "        Scale(Config.sample_size),\n",
    "        CenterCrop(Config.sample_size),\n",
    "        ToTensor(Config.norm_value), norm_method\n",
    "    ])\n",
    "    #temporal_transform = LoopPadding(opt.sample_duration)\n",
    "    temporal_transform = TemporalCenterCrop(Config.sample_duration, Config.downsample)\n",
    "    target_transform = ClassLabel()\n",
    "    validation_data = get_validation_set(spatial_transform, temporal_transform, target_transform)\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        validation_data,\n",
    "        batch_size=Config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=Config.n_threads,\n",
    "        pin_memory=True)\n",
    "    val_logger = Logger(\n",
    "        os.path.join(Config.result_path, 'DenseNet'+ str(Config.arch_type) + 'val.log'), ['epoch', 'loss', 'prec1', 'prec5'])\n",
    "\n",
    "best_prec1 = 0\n",
    "if Config.resume_path:\n",
    "    print('loading checkpoint {}'.format(Config.resume_path))\n",
    "    checkpoint = torch.load(Config.resume_path)\n",
    "    assert Config.arch == checkpoint['arch']\n",
    "    best_prec1 = checkpoint['best_prec1']\n",
    "    Config.begin_epoch = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "for i in range(Config.begin_epoch, Config.n_epochs + 1):\n",
    "    if Config.train:\n",
    "        adjust_learning_rate(optimizer, i)\n",
    "        \n",
    "        train_epoch(i, train_loader, model, criterion, optimizer,\n",
    "                        train_logger, train_batch_logger)\n",
    "        state = {\n",
    "            'epoch': i,\n",
    "            'arch': Config.arch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'best_prec1': best_prec1\n",
    "            }\n",
    "        save_checkpoint(state, False, store_name)\n",
    "\n",
    "    if  Config.validation:\n",
    "        validation_loss, prec1 = val_epoch(i, val_loader, model, criterion,\n",
    "            val_logger)\n",
    "                \n",
    "        is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "        state = {\n",
    "            'epoch': i,\n",
    "            'arch': Config.arch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'best_prec1': best_prec1\n",
    "            }\n",
    "        save_checkpoint(state, is_best, store_name)\n",
    "\n",
    "if Config.test:\n",
    "    # evaluate_model(densenet)\n",
    "    spatial_transform = Compose([\n",
    "        Scale(int(Config.sample_size / Config.scale_in_test)),\n",
    "        CornerCrop(Config.sample_size, Config.crop_position_in_test),\n",
    "        ToTensor(Config.norm_value), norm_method\n",
    "    ])\n",
    "    # temporal_transform = LoopPadding(opt.sample_duration, opt.downsample)\n",
    "    temporal_transform = TemporalRandomCrop(Config.sample_duration, Config.downsample)\n",
    "    target_transform = VideoID()\n",
    "\n",
    "    test_data = get_test_set(spatial_transform, temporal_transform, target_transform)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_data,\n",
    "        batch_size=Config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=Config.n_threads,\n",
    "        pin_memory=True)\n",
    "    test(test_loader, model, test_data.class_names)\n",
    "\n",
    "time_elapsed = datetime.now() - start \n",
    "print('Time elapsed (hh:mm:ss.ms) {}'.format(time_elapsed))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4aa70faba2784c160e473310a41bcaa90651d35b4fe74a12e56bb95cad485d51"
  },
  "kernelspec": {
   "display_name": "Python 3.6.5 64-bit ('densenet': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
